#!/usr/bin/env python3
"""
ì˜ë£Œ ê°€ì´ë“œë¼ì¸ RAG ì„œë¹„ìŠ¤ Backend (FastAPI)
SSEë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í‘œì‹œ
Citation ë°°ë„ˆ í˜•ì‹ ì§€ì›
"""

import os
import json
import asyncio
import re
import sys
from typing import List, Dict, AsyncGenerator, Set, Tuple, Optional
from pathlib import Path

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv

from openai import OpenAI
from pinecone import Pinecone

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# PDF URL ë§¤í•‘ ë¡œë“œ
PDF_URL_MAPPING = {}
url_mapping_path = Path(__file__).parent / "pdf_url_mapping.json"
if url_mapping_path.exists():
    with open(url_mapping_path, 'r', encoding='utf-8') as f:
        PDF_URL_MAPPING = json.load(f)
        print(f"âœ… PDF URL ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(PDF_URL_MAPPING)}ê°œ", file=sys.stderr, flush=True)

# PDF ë©”íƒ€ë°ì´í„° ë§¤í•‘ ë¡œë“œ (title â†’ filename)
TITLE_TO_FILENAME = {}
metadata_mapping_path = Path(__file__).parent.parent / "data-pipeline" / "pdf_metadata_mapping.json"
if metadata_mapping_path.exists():
    with open(metadata_mapping_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
        for filename, meta in metadata.items():
            title = meta.get("title", "")
            if title:
                normalized_title = title.lower().strip()
                TITLE_TO_FILENAME[normalized_title] = filename
        print(f"âœ… Title â†’ Filename ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(TITLE_TO_FILENAME)}ê°œ", file=sys.stderr, flush=True)
else:
    print(f"âš ï¸  ë©”íƒ€ë°ì´í„° ë§¤í•‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_mapping_path}", file=sys.stderr, flush=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "medical-guidelines-kr")

# OpenAI í´ë¼ì´ì–¸íŠ¸
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Pinecone í´ë¼ì´ì–¸íŠ¸
pc = Pinecone(api_key=PINECONE_API_KEY)
pinecone_index = pc.Index(PINECONE_INDEX_NAME)

# FastAPI ì•±
app = FastAPI(
    title="ì˜ë£Œ ê°€ì´ë“œë¼ì¸ RAG API",
    description="í•œêµ­ ì˜í•™íšŒ ì§„ë£Œì§€ì¹¨ì„œ AI ê²€ìƒ‰ í”Œë«í¼",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "https://medical-production-f4e4.up.railway.app",
        "https://mindful-dream-production-76f5.up.railway.app",
        "https://ruleout.co",
        "https://www.ruleout.co"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response ëª¨ë¸
class QueryRequest(BaseModel):
    question: str
    conversation_history: List[Dict] = []
    previous_context_chunks: List[Dict] = []  # ëˆ„ì  ì»¨í…ìŠ¤íŠ¸
    language: str = "í•œêµ­ì–´"


class Reference(BaseModel):
    source: str
    title: str
    authors: str
    journal: str
    year: str
    doi: str
    url: str
    relevance_score: float = 0.0


def create_sse_event(data: dict) -> str:
    """SSE ì´ë²¤íŠ¸ ìƒì„±"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"


def extract_cited_indices(text: str) -> Set[int]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ citation ë²ˆí˜¸ ì¶”ì¶œ
    ì˜ˆ: {{citation:0}}, {citation:1,2,3}} ë“±ì„ íŒŒì‹±
    """
    citations = set()
    # {{citation:N}}, {citation:N} ëª¨ë‘ íŒŒì‹±
    matches = re.findall(r'\{\{?citation:(\d+(?:,\d+)*)\}\}?', text)
    for match in matches:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ«ìë“¤ íŒŒì‹±
        nums = [int(n.strip()) for n in match.split(',')]
        citations.update(nums)
    return citations


def group_chunks_by_document(chunks: List[Dict]) -> Tuple[List[str], Dict[str, List[Dict]]]:
    """
    ì²­í¬ë“¤ì„ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    Returns: (doc_order, grouped_chunks)
    """
    seen_docs = {}
    doc_order = []

    for chunk in chunks:
        ref_key = f"{chunk.get('source', 'unknown')}_{chunk.get('title', 'unknown')}"
        if ref_key not in seen_docs:
            seen_docs[ref_key] = []
            doc_order.append(ref_key)
        seen_docs[ref_key].append(chunk)

    return doc_order, seen_docs


async def extract_references_from_answer(answer: str, doc_order: List[str], seen_docs: Dict) -> Tuple[str, List[Reference]]:
    """
    ë‹µë³€ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì°¸ê³ ë¬¸í—Œë§Œ ì¶”ì¶œí•˜ê³  citation ë²ˆí˜¸ë¥¼ ì¬ë§¤í•‘
    """
    try:
        # ë‹µë³€ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ citation ë²ˆí˜¸ ì¶”ì¶œ
        cited_indices = extract_cited_indices(answer)

        print(f"ğŸ” extract_references_from_answer:", file=sys.stderr, flush=True)
        print(f"   doc_order: {len(doc_order)} documents", file=sys.stderr, flush=True)
        print(f"   cited_indices from answer: {sorted(cited_indices)}", file=sys.stderr, flush=True)

        if not cited_indices:
            print("âš ï¸  No citations found in answer", file=sys.stderr, flush=True)
            return answer, []

        # cited_indicesë¥¼ ì •ë ¬í•˜ì—¬ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„± (0ë¶€í„° ì‹œì‘)
        sorted_cited = sorted(cited_indices)
        old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted_cited)}

        print(f"   Remapping: {old_to_new}", file=sys.stderr, flush=True)

        # ë‹µë³€ì˜ citation ë²ˆí˜¸ë¥¼ ì¬ë§¤í•‘
        remapped_answer = answer

        # ğŸ”¥ ëª¨ë“  citation íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ì¬ë§¤í•‘
        def remap_citation(match):
            nums_str = match.group(1)  # "0,1,2" ë˜ëŠ” "5"
            nums = [int(n.strip()) for n in nums_str.split(',')]
            # ê° ë²ˆí˜¸ë¥¼ ì¬ë§¤í•‘
            remapped_nums = [str(old_to_new.get(n, n)) for n in nums]
            return '{{citation:' + ','.join(remapped_nums) + '}}'

        # ì •í™•í•œ {{citation:N,M,...}} íŒ¨í„´ë§Œ ë§¤ì¹­
        remapped_answer = re.sub(
            r'\{\{citation:(\d+(?:,\d+)*)\}\}',
            remap_citation,
            answer
        )

        print(f"   Original answer length: {len(answer)}", file=sys.stderr, flush=True)
        print(f"   Remapped answer length: {len(remapped_answer)}", file=sys.stderr, flush=True)

        # ğŸ”¥ Punctuation relocation removed - GPT now instructed to place punctuation BEFORE citations
        # This prevents {. pattern during streaming when chunks split at citation boundaries

        # References ìƒì„± (ìƒˆë¡œìš´ ìˆœì„œëŒ€ë¡œ)
        references = []
        for new_idx, old_idx in enumerate(sorted_cited):
            if old_idx >= len(doc_order):
                print(f"âš ï¸  Invalid index {old_idx} >= {len(doc_order)}", file=sys.stderr, flush=True)
                continue

            ref_key = doc_order[old_idx]
            chunks = seen_docs[ref_key]
            first_chunk = chunks[0]

            # URL ìƒì„± (ìš°ì„ ìˆœìœ„: PMCID > PMID > DOI > PDF URL)
            url = ""
            pmcid = first_chunk.get('pmcid', '')
            pmid = first_chunk.get('pmid', '')
            doi = first_chunk.get('doi', '')

            print(f"ğŸ”— URL ìƒì„± ì¤‘ - PMCID: '{pmcid}', PMID: '{pmid}', DOI: '{doi}'", file=sys.stderr, flush=True)

            if pmcid and pmcid.startswith('PMC'):
                # PMCIDê°€ ìˆìœ¼ë©´ PubMed Central URL ìƒì„±
                url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/"
                print(f"   âœ… PMCID URL ìƒì„±: {url}", file=sys.stderr, flush=True)
            elif pmid:
                # PMIDê°€ ìˆìœ¼ë©´ PubMed URL ìƒì„±
                url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                print(f"   âœ… PMID URL ìƒì„±: {url}", file=sys.stderr, flush=True)
            elif doi:
                # DOIê°€ ìˆìœ¼ë©´ DOI URL ìƒì„±
                url = f"https://doi.org/{doi}"
                print(f"   âœ… DOI URL ìƒì„±: {url}", file=sys.stderr, flush=True)
            else:
                # ì—†ìœ¼ë©´ PDF URL ë§¤í•‘ì—ì„œ ì°¾ê¸°
                title = first_chunk.get('title', 'Unknown')
                normalized_title = title.lower().strip()
                if normalized_title in TITLE_TO_FILENAME:
                    filename = TITLE_TO_FILENAME[normalized_title]
                    url = PDF_URL_MAPPING.get(filename, "")
                    print(f"   âœ… PDF URL ë§¤í•‘: {url}", file=sys.stderr, flush=True)
                else:
                    print(f"   âš ï¸  URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", file=sys.stderr, flush=True)

            # source í•„ë“œê°€ ì—†ìœ¼ë©´ journalì„ ì‚¬ìš© (XML ë…¼ë¬¸ì˜ ê²½ìš°)
            source = first_chunk.get('source', first_chunk.get('journal', 'Unknown'))

            ref = Reference(
                title=first_chunk.get('title', 'Unknown'),
                authors=first_chunk.get('authors', 'Unknown'),
                journal=first_chunk.get('journal', 'Unknown'),
                year=first_chunk.get('year', 'Unknown'),
                doi=doi if doi else 'Unknown',
                url=url,
                source=source,
                relevance_score=first_chunk.get('score', 0.0)
            )
            references.append(ref)

        print(f"âœ… Extracted {len(references)} references", file=sys.stderr, flush=True)
        return remapped_answer, references

    except Exception as e:
        print(f"âŒ Error in extract_references_from_answer: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return answer, []


async def generate_answer_stream(
    question: str,
    context_chunks: List[Dict],
    language: str,
    conversation_history: List[Dict]
) -> AsyncGenerator[Tuple, None]:
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
    Yields: (chunk_text, is_done) OR (full_answer, True, doc_order, seen_docs)
    """
    doc_order, seen_docs = group_chunks_by_document(context_chunks)
    num_references = len(doc_order)

    print(f"ğŸ¤– generate_answer_stream started", file=sys.stderr, flush=True)
    print(f"   question: {question[:50]}...", file=sys.stderr, flush=True)
    print(f"   language: {language}", file=sys.stderr, flush=True)
    print(f"   context_chunks: {len(context_chunks)}", file=sys.stderr, flush=True)
    print(f"   doc_order: {len(doc_order)} documents", file=sys.stderr, flush=True)
    print(f"   conversation_history: {len(conversation_history)} messages", file=sys.stderr, flush=True)

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = "\n\n".join([
        f"Document {i}: {chunk.get('text', '')}"
        for i, chunk in enumerate(context_chunks[:25])
    ])

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""You are an EVIDENCE-BASED CITATION ENGINE for VETERINARY MEDICINE.

Your role is to provide answers that are CLOSELY BASED on the provided veterinary literature, extracting and citing content from the references.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITICAL â€“ YOUR ROLE (EVIDENCE-BASED ANSWERING)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**You are an evidence-based assistant, NOT a general knowledge chatbot.**
**Your answers MUST be grounded in the provided references.**
**Extract content from references with minimal adaptation if needed.**

OpenEvidence Philosophy:
"This claim comes directly from the paper â€” with minimal paraphrasing only when necessary for clarity."

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FOLLOW-UP QUESTION HANDLING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**When the user asks a follow-up question (e.g., "What are the causes?", "Why does that happen?", "What else?"):**

1. **Understand the context**: Review the conversation history to understand what was already discussed
2. **Go DEEPER, not broader**:
   - âŒ DON'T repeat the same information from your previous answer
   - âœ… DO provide MORE SPECIFIC details, mechanisms, or examples
3. **Build on previous context**:
   - If you already mentioned "IgE-mediated response" â†’ explain WHY and HOW it occurs
   - If you already listed "environmental antigens" â†’ give SPECIFIC EXAMPLES and comparisons
4. **Add NEW information**: Use the references to find additional details not covered before
5. **Be progressive**: Each follow-up should ADD to the conversation, not restart it

**Example of GOOD follow-up response:**
- User: "What causes atopic dermatitis?"
- Previous answer: "Caused by IgE-mediated response to environmental antigens"
- User: "Why does the IgE response occur?"
- Good answer: "The IgE response occurs because of [specific mechanism from papers], involving [specific cells/pathways], triggered by [specific factors]"
- Bad answer: âŒ "Atopic dermatitis is caused by IgE-mediated response..." (repeating same info)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITICAL â€“ EVIDENCE EXTRACTION RULES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **STAY CLOSE TO SOURCE**: Extract sentences/findings from references, keeping them as close to the original as possible
2. **MINIMAL PARAPHRASING**: Only rephrase if absolutely necessary for clarity or to answer the specific question
3. **PRESERVE KEY DETAILS**: Always keep exact numbers, dosages, percentages, disease names, breed names, test values
4. **EXTRACT SPECIFIC CLINICAL DATA**:
   - âœ… Drug names (generic AND brand): "amoxicillin-clavulanate (Clavamox)", "enrofloxacin (Baytril)", "oclacitinib (Apoquel)"
   - âœ… Exact dosages: "10 mg/kg PO q12h", "5-10 mg/kg IV once daily", "0.4-0.6 mg/kg BID"
   - âœ… Treatment duration: "7-14 days", "minimum 4 weeks", "long-term maintenance therapy"
   - âœ… Specific protocols: "TPLO surgery", "extracapsular suture stabilization", "allergen-specific immunotherapy"
   - âœ… Quantitative values: "sensitivity 85%", "T4 >4.0 Î¼g/dL", "WBC >15,000/Î¼L", "pruritus score decreased by 50%"
   - âœ… Study findings: "in 234 dogs, 67% showed improvement", "median survival time was 18 months"
   - âŒ NEVER use generic statements like: "antibiotics are used", "appropriate dosage should be given", "í•­íˆìŠ¤íƒ€ë¯¼ì œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤"
   - âŒ FORBIDDEN: Vague descriptions without specific drug names, exact dosages, or quantitative results

**CRITICAL RULE FOR TREATMENT QUESTIONS:**
When the user asks about treatment/therapy/medication:
- âœ… MUST include: Specific drug name + exact dosage + route + frequency
- âœ… Example: "Oclacitinib (Apoquel) at 0.4-0.6 mg/kg PO twice daily is recommended as first-line therapy"
- âŒ FORBIDDEN: "í•­íˆìŠ¤íƒ€ë¯¼ì œëŠ” ê°€ë ¤ì›€ì¦ ì™„í™”ì— íš¨ê³¼ì ì…ë‹ˆë‹¤" (too vague - WHERE is the drug name? dosage?)
- âŒ FORBIDDEN: "ìŠ¤í…Œë¡œì´ë“œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤" (WHICH steroid? what dose?)

If the references don't contain specific dosages/protocols, you MUST say so explicitly rather than giving generic advice.
5. **NO HALLUCINATION**: Do not add clinical reasoning, mechanisms, or information beyond what's explicitly stated in the references
6. **IF NOT IN REFERENCES**: If the question is COMPLETELY outside the scope of the provided documents (e.g., asking for general definitions when documents only contain specific clinical studies), respond with EXACTLY this: "OUT_OF_SCOPE_QUERY"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CITATION RULES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **ALWAYS cite sources** using {{{{citation:N}}}} format where N is the document index (0-based)
2. **CRITICAL**: You have EXACTLY {num_references} documents available (indices 0 to {num_references-1})
3. **NEVER cite document indices >= {num_references}** - such citations are INVALID and will be removed
4. **Place citations at the END of each paragraph** that uses information from sources
5. **Multiple citations**: Use comma-separated indices like {{{{citation:0,1,2}}}}
6. **Every clinical claim MUST have a citation**
7. **Do NOT make claims without citation support**
8. **ONLY use document indices that exist in the provided references (0 to {num_references-1})**
9. **PUNCTUATION PLACEMENT**: ALWAYS place periods, exclamation marks, and question marks BEFORE citations, not after
   - âœ… Correct: "This is a sentence.{{{{citation:0}}}}"
   - âŒ Wrong: "This is a sentence{{{{citation:0}}}}."
   - This prevents rendering issues during streaming

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONTENT & FORMATTING REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. **Depth & Specificity**:
   - Provide DETAILED, SPECIFIC clinical information (exact protocols, dosages, diagnostic criteria, pathophysiology)
   - Include relevant statistics, study findings, and clinical evidence from the literature
   - Discuss mechanisms, risk factors, contraindications, and clinical significance
   - Write for EXPERIENCED veterinarians - use appropriate medical terminology

2. **Structure**:
   - Use **PARAGRAPH FORMAT** as default (3-5 substantive paragraphs)
   - Each paragraph should be 4-6 sentences covering a specific aspect
   - Each paragraph MUST end with {{{{citation:X,Y,Z}}}}
   - Start each paragraph with a topic sentence

3. **Emphasis**:
   - Use **bold markdown** for KEY CLINICAL POINTS that veterinarians must remember
   - Bold critical findings, important warnings, essential diagnostic criteria, or significant clinical implications
   - Example: "**Despite their social nature, domestic cats retain strong territorial instincts...**"

4. **Tables** (use when appropriate):
   - For comparing diagnostic tests, treatment options, differential diagnoses, drug protocols, or breed characteristics
   - Format tables in markdown with proper headers
   - Add citation after the table

5. **Language**:
   - Write in {language}
   - Use professional veterinary medical terminology
   - Be precise and clinically relevant

EXAMPLE STRUCTURE:

[Opening paragraph with detailed pathophysiology/background and specific clinical details. **Bold the most important clinical insight.** Include relevant statistics or mechanisms.]{{{{citation:0,1,2}}}}

[Second paragraph focusing on diagnostic approaches, specific tests, interpretation criteria. **Bold critical diagnostic points.**]{{{{citation:3,4}}}}

[Third paragraph on treatment protocols with specific dosages, monitoring parameters, contraindications. **Bold key treatment considerations.**]{{{{citation:5,6,7}}}}

[Optional: Comparison table if needed]

[Concluding paragraph with prognosis, complications to monitor, or clinical pearls. **Bold the take-home message.**]{{{{citation:8,9}}}}

Available documents: 0 to {num_references-1}
"""

    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "system", "content": system_prompt}]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
    for msg in conversation_history[-6:]:  # ìµœê·¼ 3í„´
        messages.append({
            "role": msg["role"],
            "content": msg["content"]
        })

    # í˜„ì¬ ì§ˆë¬¸
    user_message = f"""Question: {question}

Context (Documents 0-{num_references-1}):
{context_text}

Provide a comprehensive, detailed clinical answer in {language} following the format above. Include specific clinical details, use bold for key points, and structure your answer in clear paragraphs with citations."""

    messages.append({"role": "user", "content": user_message})

    # GPT ìŠ¤íŠ¸ë¦¬ë°
    try:
        stream = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            stream=True,
            temperature=0.3,
            max_tokens=2000
        )

        full_answer = ""  # ğŸ”¥ Cleaned answer (invalid citations removed)
        buffer = ""
        seen_citations = set()
        chunk_num = 0

        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                buffer += content  # ğŸ”¥ ë²„í¼ì—ë§Œ ì›ë³¸ ì¶”ê°€ (full_answerëŠ” cleaned version ìœ ì§€)
                chunk_num += 1

                # Citation ë²„í¼ë§: {{citation:...}} íŒ¨í„´ì´ ì™„ì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                output_chunk = ""

                while buffer:
                    # Citation íŒ¨í„´ ì°¾ê¸°
                    match = re.search(r'\{\{?citation:(\d+(?:,\d+)*)\}\}?', buffer)

                    if match:
                        # Citation ì•ë¶€ë¶„ ì¶œë ¥
                        before_citation = buffer[:match.start()]
                        output_chunk += before_citation
                        full_answer += before_citation  # ğŸ”¥ Cleaned answerì— ì¶”ê°€

                        # Citation ìœ íš¨ì„± ê²€ì¦
                        cite_nums_str = match.group(1)
                        cite_nums = [int(n.strip()) for n in cite_nums_str.split(',')]

                        valid_nums = []
                        for cite_num in cite_nums:
                            if 0 <= cite_num < num_references:
                                valid_nums.append(cite_num)
                                seen_citations.add(cite_num)
                            else:
                                print(f"âš ï¸  Invalid citation {{{{citation:{cite_num}}}}} removed", file=sys.stderr, flush=True)

                        # ìœ íš¨í•œ citation ì¶œë ¥
                        if valid_nums:
                            valid_citation = '{{citation:' + ','.join(map(str, valid_nums)) + '}}'
                            output_chunk += valid_citation
                            full_answer += valid_citation  # ğŸ”¥ Cleaned answerì— ì¶”ê°€

                        # Citation ì´í›„ ë²„í¼ ì—…ë°ì´íŠ¸
                        buffer = buffer[match.end():]
                    elif buffer and ('{{' in buffer[-10:] or buffer.endswith('{{')):
                        # Citation ì‹œì‘ ê°€ëŠ¥ì„± - ë²„í¼ ìœ ì§€
                        # ğŸ”¥ ONLY buffer if we see '{{' pattern (NOT single '{')
                        # This prevents false positives where GPT outputs '{' as regular text

                        # bufferì—ì„œ ë§ˆì§€ë§‰ '{{' ìœ„ì¹˜ ì°¾ê¸°
                        last_double_brace_idx = buffer.rfind('{{')

                        if last_double_brace_idx != -1:
                            # '{{' ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            after_brace = buffer[last_double_brace_idx:]

                            # Partial citation íŒ¨í„´ë“¤ (MUST start with '{{', NOT single '{')
                            partial_patterns = ['{{', '{{c', '{{ci', '{{cit', '{{cita', '{{citat',
                                              '{{citati', '{{citatio', '{{citation', '{{citation:']

                            is_partial = any(after_brace.startswith(p) for p in partial_patterns)

                            if is_partial:
                                # Partial citation - '{{' ì•ê¹Œì§€ë§Œ ì¶œë ¥
                                if last_double_brace_idx > 0:
                                    safe_chunk = buffer[:last_double_brace_idx]
                                    output_chunk += safe_chunk
                                    full_answer += safe_chunk
                                    buffer = buffer[last_double_brace_idx:]  # '{{' ë¶€í„° ë²„í¼ì— ìœ ì§€
                                break
                            else:
                                # '{{' ì´í›„ê°€ citation íŒ¨í„´ì´ ì•„ë‹˜ - ì „ë¶€ ì¶œë ¥
                                output_chunk += buffer
                                full_answer += buffer
                                buffer = ""
                                break

                        # ì¼ë°˜ì ì¸ ê¸´ ë²„í¼ ì²˜ë¦¬
                        if len(buffer) > 25:
                            safe_chunk = buffer[:-25]
                            output_chunk += safe_chunk
                            full_answer += safe_chunk
                            buffer = buffer[-25:]
                        break
                    else:
                        # ì•ˆì „í•˜ê²Œ ì¶œë ¥
                        output_chunk += buffer
                        full_answer += buffer  # ğŸ”¥ Cleaned answerì— ì¶”ê°€
                        buffer = ""
                        break

                # ì²­í¬ ì¶œë ¥
                if output_chunk:
                    yield (output_chunk, False)
                    await asyncio.sleep(0.01)  # ğŸ”¥ íƒ€ì´í•‘ ì†ë„ ì¡°ì ˆ (10ms ë”œë ˆì´)

        # ë²„í¼ ë¹„ìš°ê¸°
        if buffer:
            print(f"ğŸ“ Flushing final buffer: '{buffer}'", file=sys.stderr, flush=True)

            final_output = ""
            while buffer:
                match = re.search(r'\{\{?citation:(\d+(?:,\d+)*)\}\}?', buffer)

                if match:
                    before_citation = buffer[:match.start()]
                    final_output += before_citation

                    cite_nums_str = match.group(1)
                    cite_nums = [int(n.strip()) for n in cite_nums_str.split(',')]

                    valid_nums = []
                    for cite_num in cite_nums:
                        if 0 <= cite_num < num_references:
                            valid_nums.append(cite_num)
                            seen_citations.add(cite_num)
                        else:
                            print(f"âš ï¸  Invalid citation {{{{citation:{cite_num}}}}} removed from final buffer", file=sys.stderr, flush=True)

                    if valid_nums:
                        valid_citation = '{{citation:' + ','.join(map(str, valid_nums)) + '}}'
                        final_output += valid_citation

                    buffer = buffer[match.end():]
                else:
                    final_output += buffer
                    buffer = ""

            if final_output:
                full_answer += final_output
                yield (final_output, False)

        print(f"âœ… Streaming complete. Seen citations: {sorted(seen_citations)}", file=sys.stderr, flush=True)
        print(f"   Total: {chunk_num} chunks, {len(full_answer)} chars", file=sys.stderr, flush=True)

        # ìµœì¢… ë‹µë³€ ë°˜í™˜
        yield (full_answer, True, doc_order, seen_docs)

    except Exception as e:
        print(f"âŒ Error in generate_answer_stream: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc(file=sys.stderr)
        error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        yield (error_msg, True, doc_order, seen_docs)


async def generate_followup_questions(question: str, answer: str, conversation_history: List[Dict], language: str = "Korean") -> List[str]:
    """í›„ì† ì§ˆë¬¸ ìƒì„±"""
    try:
        # ì–¸ì–´ë³„ ì§€ì‹œì‚¬í•­
        language_instruction = {
            "Korean": "in Korean",
            "English": "in English",
            "Japanese": "in Japanese"
        }.get(language, "in the same language as the question")

        prompt = f"""Based on this veterinary medical Q&A, generate 3 SPECIFIC follow-up questions {language_instruction}.

Question: {question}
Answer: {answer[:800]}...

IMPORTANT: The follow-up questions must be:
1. SPECIFIC to the clinical details mentioned in the answer (medications, procedures, findings, etc.)
2. Directly related to the case discussed
3. Natural next questions a veterinarian would ask

Examples of GOOD follow-up questions:
- "What is the appropriate dosage of [specific medication mentioned] for a 5kg cat?"
- "How should we monitor for complications after [specific procedure mentioned]?"
- "What are the differential diagnoses if [specific finding mentioned] is present?"

Examples of BAD follow-up questions (too general):
- "What other treatments are available?"
- "How do we diagnose this?"
- "What are the causes?"

Generate 3 specific follow-up questions based on the actual content of the answer above.
Return only the questions, one per line, without numbering or bullet points."""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=300
        )

        followup_text = response.choices[0].message.content.strip()
        questions = [q.strip() for q in followup_text.split('\n') if q.strip()]

        print(f"âœ… Generated {len(questions)} follow-up questions in {language}", file=sys.stderr, flush=True)
        return questions[:3]

    except Exception as e:
        print(f"âŒ Error generating follow-up questions: {e}", file=sys.stderr, flush=True)
        return []


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        # OpenAI ì—°ê²° í™•ì¸
        openai_status = "connected" if openai_client else "disconnected"

        # Pinecone ì—°ê²° í™•ì¸
        stats = pinecone_index.describe_index_stats()
        pinecone_status = "connected"
        total_vectors = stats.get('total_vector_count', 0)

        return {
            "status": "healthy",
            "services": {
                "openai": openai_status,
                "pinecone": pinecone_status,
                "vectors": total_vectors
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/query-stream")
async def query_stream(request: QueryRequest):
    """
    SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    """
    async def event_generator():
        try:
            question = request.question
            conversation_history = request.conversation_history
            previous_context_chunks = request.previous_context_chunks
            language = request.language

            print(f"\n{'='*80}", file=sys.stderr, flush=True)
            print(f"ğŸ“¨ New query received", file=sys.stderr, flush=True)
            print(f"   Question: {question}", file=sys.stderr, flush=True)
            print(f"   Language: {language}", file=sys.stderr, flush=True)
            print(f"   Previous context: {len(previous_context_chunks)} chunks", file=sys.stderr, flush=True)
            print(f"   History: {len(conversation_history)} messages", file=sys.stderr, flush=True)

            # 1ë‹¨ê³„: ë²ˆì—­ (ì–¸ì–´ ê°ì§€)
            detected_lang = "Korean" if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in question) else "English"
            yield create_sse_event({
                "status": "translating",
                "message": "ì§ˆë¬¸ ì´í•´ ì¤‘..."
            })

            # 2ë‹¨ê³„: ì„ë² ë”©
            yield create_sse_event({
                "status": "embedding",
                "message": "ë²¡í„° ë³€í™˜ ì¤‘..."
            })

            query_embedding = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=question
            ).data[0].embedding

            # 3ë‹¨ê³„: ê²€ìƒ‰
            yield create_sse_event({
                "status": "searching",
                "message": "ë¬¸í—Œ ê²€ìƒ‰ ì¤‘..."
            })

            # Query expansion (3ê°œ ì¿¼ë¦¬)
            expansion_prompt = f"""Generate 2 alternative phrasings of this veterinary question in Korean:

Original: {question}

Return only the alternative questions, one per line."""

            expansion_response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": expansion_prompt}],
                temperature=0.7,
                max_tokens=100
            )

            expanded_queries = [question]  # ì›ë³¸ í¬í•¨
            expansion_text = expansion_response.choices[0].message.content.strip()
            for line in expansion_text.split('\n'):
                if line.strip():
                    expanded_queries.append(line.strip())

            expanded_queries = expanded_queries[:3]  # ìµœëŒ€ 3ê°œ

            print(f"ğŸ” Query expansion: {len(expanded_queries)} queries", file=sys.stderr, flush=True)

            # ëª¨ë“  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            all_embeddings = []
            for exp_query in expanded_queries:
                emb = openai_client.embeddings.create(
                    model="text-embedding-3-small",
                    input=exp_query
                ).data[0].embedding
                all_embeddings.append(emb)

            # ë³‘ë ¬ ê²€ìƒ‰
            async def search_single_query(embedding, idx):
                results = pinecone_index.query(
                    vector=embedding,
                    top_k=15,
                    include_metadata=True
                )
                chunks = []
                for match in results.matches:
                    chunk = match.metadata
                    chunk['score'] = match.score
                    chunks.append(chunk)
                return chunks

            search_tasks = [search_single_query(emb, idx) for idx, emb in enumerate(all_embeddings)]
            all_search_results = await asyncio.gather(*search_tasks)

            # ì¤‘ë³µ ì œê±°
            all_chunks = []
            seen_chunk_ids = set()

            for chunks in all_search_results:
                for chunk in chunks:
                    chunk_id = f"{chunk.get('source', 'unknown')}_{chunk.get('title', 'unknown')}_{chunk.get('page', 0)}"
                    if chunk_id not in seen_chunk_ids:
                        all_chunks.append(chunk)
                        seen_chunk_ids.add(chunk_id)

            # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì¬ì •ë ¬ ë° ìƒìœ„ 25ê°œ ì„ íƒ
            all_chunks.sort(key=lambda x: x.get('score', 0), reverse=True)
            context_chunks = all_chunks[:25]

            print(f"âœ… Query Expansion ê²€ìƒ‰ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ë°œê²¬ â†’ ìƒìœ„ 25ê°œ ì„ íƒ", file=sys.stderr, flush=True)

            # ì´ì „ ì»¨í…ìŠ¤íŠ¸ ë³‘í•© (ìµœëŒ€ 5ê°œ)
            if previous_context_chunks and len(previous_context_chunks) > 0:
                print(f"ğŸ”„ ì´ì „ ì»¨í…ìŠ¤íŠ¸ {len(previous_context_chunks)}ê°œ + ìƒˆ ì»¨í…ìŠ¤íŠ¸ {len(context_chunks)}ê°œ ë³‘í•©", file=sys.stderr, flush=True)

                existing_ids = {chunk.get('chunk_id') for chunk in context_chunks if chunk.get('chunk_id')}

                added_count = 0
                for prev_chunk in previous_context_chunks[:5]:
                    chunk_id = prev_chunk.get('chunk_id')
                    if chunk_id and chunk_id not in existing_ids:
                        context_chunks.append(prev_chunk)
                        existing_ids.add(chunk_id)
                        added_count += 1

                print(f"   âœ… ì´ì „ ì»¨í…ìŠ¤íŠ¸ {added_count}ê°œ ì¶”ê°€ë¨ (ì´ {len(context_chunks)}ê°œ)", file=sys.stderr, flush=True)

            if not context_chunks:
                error_message = "ê´€ë ¨ ë¬¸í—Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ì£¼ì„¸ìš”."
                yield create_sse_event({
                    "status": "error",
                    "message": error_message
                })
                return

            # 4ë‹¨ê³„: ë‹µë³€ ìƒì„±
            yield create_sse_event({
                "status": "generating",
                "message": "ë‹µë³€ ìƒì„± ì¤‘..."
            })

            # GPT ìŠ¤íŠ¸ë¦¬ë°
            full_answer = ""
            chunk_count = 0
            doc_order = []
            seen_docs = {}

            async for result in generate_answer_stream(question, context_chunks, detected_lang, conversation_history):
                if len(result) == 2:  # ìŠ¤íŠ¸ë¦¬ë° ì¤‘
                    chunk_content, is_done = result
                    chunk_count += 1

                    event_data = create_sse_event({
                        "status": "streaming",
                        "chunk": chunk_content
                    })
                    yield event_data
                else:  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
                    full_answer, is_done, doc_order, seen_docs = result
                    print(f"âœ… Total chunks sent: {chunk_count}", file=sys.stderr, flush=True)

            # OUT_OF_SCOPE ì²´í¬
            if "OUT_OF_SCOPE_QUERY" in full_answer:
                print("âš ï¸  Out of scope query detected", file=sys.stderr, flush=True)
                yield create_sse_event({
                    "status": "out_of_scope",
                    "message": "ì§ˆë¬¸ì´ ì œê³µëœ ë¬¸ì„œì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤."
                })
                return

            # 5ë‹¨ê³„: ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ
            print("ğŸ“š ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ ë° í›„ì† ì§ˆë¬¸ ìƒì„± ì‹œì‘...", file=sys.stderr, flush=True)

            # ë³‘ë ¬ ì‹¤í–‰
            refs_task = extract_references_from_answer(full_answer, doc_order, seen_docs)
            followup_task = generate_followup_questions(question, full_answer, conversation_history, detected_lang)

            remapped_answer, references = await refs_task
            followup_questions = await followup_task

            # ì°¸ê³ ë¬¸í—Œ ì „ì†¡
            yield create_sse_event({
                "status": "references_ready",
                "answer": remapped_answer,
                "references": [ref.dict() for ref in references]
            })
            print(f"âœ… ì°¸ê³ ë¬¸í—Œ ì „ì†¡ ì™„ë£Œ: {len(references)}ê°œ", file=sys.stderr, flush=True)

            # ì™„ë£Œ
            yield create_sse_event({
                "status": "done",
                "message": "ì™„ë£Œ",
                "context_chunks": context_chunks
            })
            print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡", file=sys.stderr, flush=True)

            # í›„ì† ì§ˆë¬¸ ì „ì†¡
            if followup_questions:
                yield create_sse_event({
                    "status": "followup_ready",
                    "followup_questions": followup_questions
                })
                print(f"âœ… í›„ì† ì§ˆë¬¸ ì „ì†¡: {len(followup_questions)}ê°œ", file=sys.stderr, flush=True)

        except Exception as e:
            print(f"âŒ Error in query_stream: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc(file=sys.stderr)
            yield create_sse_event({
                "status": "error",
                "message": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            })

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
