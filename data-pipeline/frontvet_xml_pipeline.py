"""
Frontiers in Veterinary Science XML ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
PMC XML íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ Pinecone ë²¡í„° DBì— ì €ì¥
"""

import os
import re
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone
import json

load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("medical-guidelines")


def extract_text_from_element(element):
    """XML ìš”ì†Œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if element is None:
        return ""

    text_parts = []
    if element.text:
        text_parts.append(element.text.strip())

    for child in element:
        child_text = extract_text_from_element(child)
        if child_text:
            text_parts.append(child_text)
        if child.tail:
            text_parts.append(child.tail.strip())

    return " ".join(text_parts)


def extract_authors(contrib_group):
    """ì €ì ì •ë³´ ì¶”ì¶œ (6ì¸ê¹Œì§€, ì´í›„ et al.)"""
    if contrib_group is None:
        return ""

    authors = []
    for contrib in contrib_group.findall('.//contrib[@contrib-type="author"]'):
        name_elem = contrib.find('.//name')
        if name_elem is not None:
            surname = name_elem.find('surname')
            given_names = name_elem.find('given-names')

            if surname is not None:
                surname_text = surname.text if surname.text else ""
                given_text = ""
                if given_names is not None and given_names.text:
                    # ì´ë‹ˆì…œë§Œ ì¶”ì¶œ
                    initials = ''.join([c[0] for c in given_names.text.split() if c])
                    given_text = initials

                if surname_text:
                    author = f"{surname_text} {given_text}".strip()
                    authors.append(author)

    # 6ì¸ê¹Œì§€ë§Œ í‘œê¸°, ì´í›„ et al.
    if len(authors) > 6:
        return ", ".join(authors[:6]) + ", et al."
    else:
        return ", ".join(authors) + "."


def extract_metadata_from_xml(xml_path: Path) -> Dict:
    """PMC XMLì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬
        ns = {'': 'http://www.w3.org/1998/Math/MathML'}

        # article-metaì—ì„œ ì •ë³´ ì¶”ì¶œ
        article_meta = root.find('.//article-meta')
        if article_meta is None:
            return None

        # PMCID
        pmcid_elem = article_meta.find('.//article-id[@pub-id-type="pmcid"]')
        pmcid = pmcid_elem.text if pmcid_elem is not None else xml_path.stem

        # PMID
        pmid_elem = article_meta.find('.//article-id[@pub-id-type="pmid"]')
        pmid = pmid_elem.text if pmid_elem is not None else ""

        # DOI
        doi_elem = article_meta.find('.//article-id[@pub-id-type="doi"]')
        doi = doi_elem.text if doi_elem is not None else ""

        # Title
        title_elem = article_meta.find('.//article-title')
        title = extract_text_from_element(title_elem) if title_elem is not None else ""

        # Authors
        contrib_group = article_meta.find('.//contrib-group')
        authors = extract_authors(contrib_group)

        # Journal
        journal_meta = root.find('.//journal-meta')
        journal_title = ""
        if journal_meta is not None:
            journal_elem = journal_meta.find('.//journal-title')
            if journal_elem is not None:
                journal_title = journal_elem.text

        # Publication date
        pub_date = article_meta.find('.//pub-date[@pub-type="epub"]')
        if pub_date is None:
            pub_date = article_meta.find('.//pub-date[@pub-type="collection"]')

        year = ""
        if pub_date is not None:
            year_elem = pub_date.find('year')
            month_elem = pub_date.find('month')
            day_elem = pub_date.find('day')

            year_text = year_elem.text if year_elem is not None else ""
            month_text = month_elem.text if month_elem is not None else ""
            day_text = day_elem.text if day_elem is not None else ""

            if year_text:
                year = f"{year_text}"
                if month_text:
                    year += f" {month_text}"
                if day_text:
                    year += f" {day_text}"

        return {
            "pmcid": pmcid,
            "pmid": pmid,
            "doi": doi,
            "title": title,
            "authors": authors,
            "journal": journal_title if journal_title else "Front Vet Sci",
            "year": year,
            "filename": xml_path.name
        }

    except Exception as e:
        print(f"  âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def extract_full_text_from_xml(xml_path: Path) -> str:
    """XMLì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # body ì„¹ì…˜ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        body = root.find('.//body')
        if body is None:
            return ""

        full_text = extract_text_from_element(body)

        # ì¤‘ë³µ ê³µë°± ì œê±°
        full_text = re.sub(r'\s+', ' ', full_text).strip()

        return full_text

    except Exception as e:
        print(f"  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""


def recursive_chunk_with_overlap(text: str, chunk_size: int = 600, overlap: int = 150) -> List[str]:
    """ì¬ê·€ì  ì²­í‚¹ (ì˜¤ë²„ë© í¬í•¨)"""
    if not text or len(text) == 0:
        return []

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # ë§ˆì§€ë§‰ ì²­í¬
        if end >= len(text):
            chunk = text[start:].strip()
            if len(chunk) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                chunks.append(chunk)
            break

        # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
        chunk = text[start:end]

        # ë¬¸ì¥ ë ì°¾ê¸° (., !, ?)
        sentence_end = max(
            chunk.rfind('. '),
            chunk.rfind('! '),
            chunk.rfind('? ')
        )

        if sentence_end > chunk_size * 0.5:
            chunk = text[start:start + sentence_end + 1].strip()
            chunks.append(chunk)
            start = start + sentence_end + 1 - overlap
        else:
            chunk = text[start:end].strip()
            chunks.append(chunk)
            start = end - overlap

    return chunks


def create_embeddings(texts: List[str]) -> List[List[float]]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    embeddings = []
    batch_size = 100

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        response = client.embeddings.create(
            input=batch,
            model="text-embedding-3-small"
        )
        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)

    return embeddings


def process_single_xml(xml_path: Path) -> int:
    """ë‹¨ì¼ XML íŒŒì¼ ì²˜ë¦¬"""
    print(f"\n{'='*60}")
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {xml_path.name}")
    print(f"{'='*60}")

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    print(f"  âš™ï¸  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    metadata = extract_metadata_from_xml(xml_path)
    if metadata is None:
        print(f"  âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")
        return 0

    print(f"  ğŸ“‹ ì œëª©: {metadata['title'][:60]}...")
    print(f"  ğŸ‘¥ ì €ì: {metadata['authors'][:60]}...")
    print(f"  ğŸ“… ì—°ë„: {metadata['year']}")
    print(f"  ğŸ”– PMCID: {metadata['pmcid']}")

    # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print(f"  âš™ï¸  ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    full_text = extract_full_text_from_xml(xml_path)
    if not full_text:
        print(f"  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return 0

    # ì²­í‚¹
    print(f"  âš™ï¸  ì²­í‚¹ ì¤‘...")
    chunks = recursive_chunk_with_overlap(full_text, chunk_size=600, overlap=150)
    print(f"  ğŸ“¦ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

    if len(chunks) == 0:
        print(f"  âš ï¸  ì²­í¬ ì—†ìŒ")
        return 0

    # Reference í˜•ì‹ ìƒì„± (PDF ìŠ¤íƒ€ì¼ê³¼ ë™ì¼)
    reference = f"{metadata['authors']} {metadata['journal']}. {metadata['year']}. doi:{metadata['doi']}"

    # ìƒ˜í”Œ ì²­í¬ ì¶œë ¥
    print(f"\n  {'â”€'*58}")
    print(f"  ğŸ“‹ ìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ 2ê°œ)")
    print(f"  {'â”€'*58}")
    for i in range(min(2, len(chunks))):
        print(f"\n  [ì²­í¬ {i+1}]")
        print(f"  í…ìŠ¤íŠ¸: {chunks[i][:100]}...")
        print(f"  Reference: {reference}")

    # ì„ë² ë”© ìƒì„±
    print(f"\n  âš™ï¸  ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = create_embeddings(chunks)
    print(f"  ğŸ“Š ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")

    # Pineconeì— ì €ì¥
    print(f"  âš™ï¸  Pineconeì— ì €ì¥ ì¤‘...")
    vectors = []
    pmcid = metadata['pmcid']

    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        vector_id = f"frontvet_{pmcid}_c{i}"

        vector_metadata = {
            "text": chunk,
            "source": "frontiers_veterinary",
            "pmcid": metadata['pmcid'],
            "pmid": metadata['pmid'],
            "doi": metadata['doi'],
            "title": metadata['title'],
            "authors": metadata['authors'],
            "journal": metadata['journal'],
            "year": metadata['year'],
            "reference": reference,
            "chunk_index": i,
            "filename": metadata['filename']
        }

        vectors.append({
            "id": vector_id,
            "values": embedding,
            "metadata": vector_metadata
        })

    # ë°°ì¹˜ë¡œ ì—…ë¡œë“œ
    batch_size = 100
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i + batch_size]
        index.upsert(vectors=batch)
        print(f"  ğŸ’¾ Pinecone ì €ì¥: {i+1}-{min(i+batch_size, len(vectors))}/{len(vectors)}")

    print(f"  âœ… ì™„ë£Œ! {len(chunks)}ê°œ ì²­í¬ ì €ì¥")

    return len(chunks)


def process_all_xmls(xml_dir: Path, start_from: int = 0):
    """ëª¨ë“  XML íŒŒì¼ ì²˜ë¦¬"""
    xml_files = sorted(list(xml_dir.glob("*.xml")))

    print("="*60)
    print(f"Frontiers in Veterinary Science XML ì²˜ë¦¬ ì‹œì‘")
    print("="*60)
    print(f"ì´ XML íŒŒì¼: {len(xml_files)}ê°œ")
    print(f"ì²˜ë¦¬ ì‹œì‘ ìœ„ì¹˜: {start_from}")
    print("="*60)

    total_chunks = 0
    processed_count = 0
    failed_count = 0

    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ
    progress_file = Path("frontvet_processing_progress.json")
    processed_files = set()
    if progress_file.exists():
        with open(progress_file, 'r') as f:
            data = json.load(f)
            processed_files = set(data.get('processed_files', []))

    for idx, xml_path in enumerate(xml_files[start_from:], start=start_from):
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì€ ìŠ¤í‚µ
        if xml_path.name in processed_files:
            print(f"\nâ­ï¸  ìŠ¤í‚µ: {xml_path.name} (ì´ë¯¸ ì²˜ë¦¬ë¨)")
            continue

        try:
            chunks_count = process_single_xml(xml_path)
            if chunks_count > 0:
                total_chunks += chunks_count
                processed_count += 1
                processed_files.add(xml_path.name)

                # ì§„í–‰ìƒí™© ì €ì¥ (10ê°œë§ˆë‹¤)
                if processed_count % 10 == 0:
                    with open(progress_file, 'w') as f:
                        json.dump({
                            'processed_files': list(processed_files),
                            'total_processed': processed_count,
                            'total_chunks': total_chunks
                        }, f)
            else:
                failed_count += 1

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            failed_count += 1
            continue

        # ì§„í–‰ìƒí™© ì¶œë ¥ (50ê°œë§ˆë‹¤)
        if (idx + 1) % 50 == 0:
            print(f"\n{'='*60}")
            print(f"ì§„í–‰ìƒí™©: {processed_count}/{len(xml_files)} ì™„ë£Œ")
            print(f"ì´ ì²­í¬: {total_chunks:,}ê°œ")
            print(f"ì‹¤íŒ¨: {failed_count}ê°œ")
            print(f"{'='*60}\n")

    # ìµœì¢… ì§„í–‰ìƒí™© ì €ì¥
    with open(progress_file, 'w') as f:
        json.dump({
            'processed_files': list(processed_files),
            'total_processed': processed_count,
            'total_chunks': total_chunks
        }, f)

    print(f"\n{'='*60}")
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"  - ì²˜ë¦¬ëœ íŒŒì¼: {processed_count}ê°œ")
    print(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"  - ì´ ì²­í¬: {total_chunks:,}ê°œ")
    print(f"{'='*60}")


if __name__ == "__main__":
    xml_dir = Path("guidelines/xml_frontvet")
    process_all_xmls(xml_dir, start_from=0)
