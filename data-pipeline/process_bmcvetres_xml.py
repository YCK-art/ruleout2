"""
BMC Veterinary Research XML ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
PMC XML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ Pinecone ë²¡í„° DBì— ì²­í‚¹ ë° ì„ë² ë”© ì €ì¥
ì§„í–‰ ìƒí™© ìë™ ì €ì¥ (10ê°œë§ˆë‹¤)
"""

import os
import re
import json
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Optional
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone
import sys

load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("medical-guidelines")

# ì§„í–‰ ìƒí™© íŒŒì¼
PROGRESS_FILE = Path("/Users/ksinfosys/medical/data-pipeline/bmcvetres_processing_progress.json")


# ============================================================
# ì§„í–‰ ìƒí™© ê´€ë¦¬
# ============================================================

def load_progress():
    """ì§„í–‰ ìƒí™© ë¡œë“œ"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "processed_files": [],
        "total_processed": 0,
        "total_chunks": 0
    }


def save_progress(progress):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)
    print(f"\n  ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥ë¨: {progress['total_processed']}ê°œ íŒŒì¼, {progress['total_chunks']}ê°œ ì²­í¬")


# ============================================================
# Step 1: XML ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
# ============================================================

def extract_text_from_element(element, default=""):
    """XML ì—˜ë¦¬ë¨¼íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if element is None:
        return default
    text = ''.join(element.itertext()).strip()
    return text if text else default


def extract_xml_metadata(xml_path: Path) -> Dict:
    """PMC XMLì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""

    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # Namespace ì²˜ë¦¬ (PMC XMLì€ ë³´í†µ namespaceê°€ ìˆìŒ)
        namespaces = {'': 'http://www.w3.org/1998/Math/MathML'} if root.tag.startswith('{') else {}

        # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        metadata = {
            "title": "",
            "authors": "",
            "journal": "",
            "year": "",
            "doi": "",
            "pmcid": "",
            "pmid": "",
            "abstract": ""
        }

        # PMCID ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
        pmcid_match = re.search(r'PMC\d+', xml_path.name)
        if pmcid_match:
            metadata["pmcid"] = pmcid_match.group(0)

        # article-meta ì°¾ê¸°
        article_meta = root.find('.//article-meta')
        if article_meta is None:
            print(f"  âš ï¸  article-metaë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return metadata

        # ì œëª© ì¶”ì¶œ
        title_elem = article_meta.find('.//article-title')
        if title_elem is not None:
            metadata["title"] = extract_text_from_element(title_elem)

        # ì €ì ì¶”ì¶œ
        authors = []
        for contrib in article_meta.findall('.//contrib[@contrib-type="author"]'):
            name_elem = contrib.find('.//name')
            if name_elem is not None:
                surname = extract_text_from_element(name_elem.find('surname'))
                given = extract_text_from_element(name_elem.find('given-names'))
                if surname:
                    author_name = f"{given} {surname}" if given else surname
                    authors.append(author_name)

        if authors:
            if len(authors) <= 6:
                # ì €ìê°€ 6ëª… ì´í•˜ â†’ ì „ì› í‘œê¸°
                metadata["authors"] = ", ".join(authors)
            else:
                # ì €ìê°€ 7ëª… ì´ìƒ â†’ ì• 6ëª…ë§Œ í‘œê¸°í•˜ê³  et al.
                metadata["authors"] = ", ".join(authors[:6]) + ", et al."

        # ì €ë„ëª… ì¶”ì¶œ
        journal_elem = root.find('.//journal-title')
        if journal_elem is not None:
            metadata["journal"] = extract_text_from_element(journal_elem)

        # ì—°ë„ ì¶”ì¶œ
        year_elem = article_meta.find('.//pub-date[@pub-type="epub"]/year')
        if year_elem is None:
            year_elem = article_meta.find('.//pub-date/year')
        if year_elem is not None:
            metadata["year"] = extract_text_from_element(year_elem)

        # DOI ì¶”ì¶œ
        doi_elem = article_meta.find('.//article-id[@pub-id-type="doi"]')
        if doi_elem is not None:
            metadata["doi"] = extract_text_from_element(doi_elem)

        # PMID ì¶”ì¶œ
        pmid_elem = article_meta.find('.//article-id[@pub-id-type="pmid"]')
        if pmid_elem is not None:
            metadata["pmid"] = extract_text_from_element(pmid_elem)

        # ì´ˆë¡ ì¶”ì¶œ
        abstract_elem = article_meta.find('.//abstract')
        if abstract_elem is not None:
            metadata["abstract"] = extract_text_from_element(abstract_elem)

        return metadata

    except Exception as e:
        print(f"  âŒ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {
            "title": xml_path.stem,
            "authors": "",
            "journal": "",
            "year": "",
            "doi": "",
            "pmcid": "",
            "pmid": "",
            "abstract": ""
        }


def extract_xml_body_text(xml_path: Path) -> str:
    """PMC XMLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # body ì—˜ë¦¬ë¨¼íŠ¸ ì°¾ê¸°
        body_elem = root.find('.//body')
        if body_elem is None:
            print(f"  âš ï¸  ë³¸ë¬¸(body)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        body_text = extract_text_from_element(body_elem)

        return body_text

    except Exception as e:
        print(f"  âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""


# ============================================================
# Step 2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# ============================================================

def clean_xml_text(text: str) -> str:
    """XML í…ìŠ¤íŠ¸ ì •ë¦¬"""

    # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
    text = re.sub(r'\n{3,}', '\n\n', text)

    # ì—¬ëŸ¬ ê³µë°±ì„ 1ê°œë¡œ
    text = re.sub(r' {2,}', ' ', text)

    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = text.replace('\u200b', '')  # Zero-width space
    text = text.replace('\xa0', ' ')  # Non-breaking space

    return text.strip()


# ============================================================
# Step 3: ì‹œë§¨í‹± ì²­í‚¹ (PDFì™€ ë™ì¼)
# ============================================================

def recursive_chunk_with_overlap(text: str, chunk_size: int = 600, overlap: int = 150) -> List[str]:
    """Recursive Chunking with Overlap (PDF íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼)"""

    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))

        # ë¬¸ì¥ ê²½ê³„ë¡œ ì¡°ì • (í…ìŠ¤íŠ¸ ëì´ ì•„ë‹ ë•Œë§Œ)
        if end < len(text):
            found_separator = False
            for separator in ['. ', '.\n', '\n\n', '\n', ' ']:
                last_sep = text.rfind(separator, start, end)
                if last_sep > start:
                    end = last_sep + len(separator)
                    found_separator = True
                    break

            # êµ¬ë¶„ìë¥¼ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ê°•ì œë¡œ chunk_sizeë§Œí¼ ìë¥´ê¸°
            if not found_separator:
                end = start + chunk_size

        chunk = text[start:end].strip()
        if chunk and len(chunk) > 50:
            chunks.append(chunk)

        # ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ (overlap ì ìš©)
        next_start = end - overlap

        # ì§„í–‰ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ì•ìœ¼ë¡œ ì´ë™
        if next_start <= start:
            next_start = start + chunk_size

        start = next_start

        # ë¬´í•œ ë£¨í”„ ë°©ì§€
        if start >= len(text):
            break

    return chunks


# ============================================================
# Step 4: ì„ë² ë”© ë° Pinecone ì €ì¥ (PDFì™€ ë™ì¼)
# ============================================================

def create_embeddings(texts: List[str], batch_size: int = 100) -> List[List[float]]:
    """OpenAI APIë¡œ ì„ë² ë”© ìƒì„±"""

    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )

        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)

        print(f"  ğŸ“Š ì„ë² ë”© ìƒì„±: {i+1}-{i+len(batch)}/{len(texts)}")

    return all_embeddings


def upsert_to_pinecone(chunks_metadata: List[Dict], embeddings: List[List[float]], batch_size: int = 100):
    """Pineconeì— ë²¡í„° ì €ì¥ (PDF íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼)"""

    total = len(chunks_metadata)

    for i in range(0, total, batch_size):
        batch_meta = chunks_metadata[i:i + batch_size]
        batch_emb = embeddings[i:i + batch_size]

        vectors = []
        for chunk_meta, embedding in zip(batch_meta, batch_emb):
            metadata = {
                "doc_type": "paper",  # XMLì€ ëª¨ë‘ ë…¼ë¬¸
                "title": chunk_meta["title"],
                "year": chunk_meta["year"],
                "page": chunk_meta.get("chunk_index", 0),  # XMLì€ í˜ì´ì§€ ëŒ€ì‹  ì²­í¬ ì¸ë±ìŠ¤
                "text": chunk_meta["text"],
                "reference_format": chunk_meta["reference_format"],
                "authors": chunk_meta.get("authors", ""),
                "journal": chunk_meta.get("journal", ""),
                "doi": chunk_meta.get("doi", ""),
                "pmcid": chunk_meta.get("pmcid", ""),
                "pmid": chunk_meta.get("pmid", "")
            }

            vectors.append({
                "id": chunk_meta["id"],
                "values": embedding,
                "metadata": metadata
            })

        index.upsert(vectors=vectors)
        print(f"  ğŸ’¾ Pinecone ì €ì¥: {i+1}-{i+len(batch_meta)}/{total}")


# ============================================================
# Step 5: ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================

def process_single_xml(xml_path: Path) -> Dict:
    """ë‹¨ì¼ XML íŒŒì¼ ì²˜ë¦¬"""

    try:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {xml_path.name}")
        print(f"{'='*60}")
        sys.stdout.flush()

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        print("  âš™ï¸  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        sys.stdout.flush()
        metadata = extract_xml_metadata(xml_path)

        print(f"  ğŸ“‹ ì œëª©: {metadata.get('title', 'Unknown')[:60]}...")
        print(f"  âœï¸  ì €ì: {metadata.get('authors', 'Unknown')}")
        print(f"  ğŸ“° ì €ë„: {metadata.get('journal', 'Unknown')}")
        print(f"  ğŸ“… ì—°ë„: {metadata.get('year', 'Unknown')}")
        print(f"  ğŸ”— DOI: {metadata.get('doi', 'N/A')}")
        print(f"  ğŸ†” PMCID: {metadata.get('pmcid', 'N/A')}")
        sys.stdout.flush()

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("  âš™ï¸  ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        sys.stdout.flush()
        body_text = extract_xml_body_text(xml_path)

        if not body_text or len(body_text) < 100:
            print(f"  âš ï¸  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤. (ê¸¸ì´: {len(body_text)})")
            return {
                "success": False,
                "error": "ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ"
            }

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = clean_xml_text(body_text)
        print(f"  ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {len(clean_text):,}ì")
        sys.stdout.flush()

        # ì²­í¬ ë¶„í• 
        print("  âš™ï¸  í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        sys.stdout.flush()
        chunks = recursive_chunk_with_overlap(clean_text, chunk_size=600, overlap=150)
        print(f"  ğŸ“¦ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        sys.stdout.flush()

        # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        all_chunks_metadata = []

        for chunk_idx, chunk_text in enumerate(chunks):
            # ID ìƒì„± (paper_pmcid_cì¸ë±ìŠ¤)
            pmcid = metadata.get('pmcid', xml_path.stem)
            chunk_id = f"paper_{pmcid}_c{chunk_idx}"
            chunk_id = re.sub(r'[^a-zA-Z0-9_-]', '_', chunk_id)

            # Reference í˜•ì‹ ìƒì„± (PDF ìŠ¤íƒ€ì¼)
            ref_parts = []
            if metadata.get('authors'):
                ref_parts.append(metadata['authors'])
            if metadata.get('journal'):
                ref_parts.append(metadata['journal'])
            if metadata.get('year'):
                ref_parts.append(metadata['year'])

            ref_format = ". ".join(ref_parts) if ref_parts else metadata.get('title', '')[:50]
            if metadata.get('doi'):
                ref_format += f". doi:{metadata['doi']}"

            chunk_meta = {
                "id": chunk_id,
                "title": metadata.get("title", ""),
                "year": metadata.get("year", ""),
                "authors": metadata.get("authors", ""),
                "journal": metadata.get("journal", ""),
                "doi": metadata.get("doi", ""),
                "pmcid": metadata.get("pmcid", ""),
                "pmid": metadata.get("pmid", ""),
                "chunk_index": chunk_idx,
                "text": chunk_text,
                "reference_format": ref_format
            }

            all_chunks_metadata.append(chunk_meta)

        # ì„ë² ë”© ìƒì„±
        print(f"\n  âš™ï¸  ì„ë² ë”© ìƒì„± ì¤‘...")
        sys.stdout.flush()
        chunk_texts = [c["text"] for c in all_chunks_metadata]
        embeddings = create_embeddings(chunk_texts)

        # Pineconeì— ì €ì¥
        print(f"\n  âš™ï¸  Pineconeì— ì €ì¥ ì¤‘...")
        sys.stdout.flush()
        upsert_to_pinecone(all_chunks_metadata, embeddings)

        print(f"\n  âœ… ì™„ë£Œ!")
        sys.stdout.flush()

        return {
            "success": True,
            "chunks": len(all_chunks_metadata),
            "metadata": metadata
        }

    except Exception as e:
        print(f"\n  âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return {
            "success": False,
            "error": str(e)
        }


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    xml_folder = Path("/Users/ksinfosys/medical/data-pipeline/guidelines/xml_bmcvetres")

    # ì§„í–‰ ìƒí™© ë¡œë“œ
    progress = load_progress()
    processed_files_set = set(progress["processed_files"])

    print(f"\n{'='*60}")
    print(f"ğŸš€ BMC Veterinary Research XML ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ğŸ“ í´ë”: {xml_folder}")
    print(f"ğŸ’¾ ì´ì „ ì§„í–‰: {progress['total_processed']}ê°œ íŒŒì¼, {progress['total_chunks']}ê°œ ì²­í¬")
    print(f"{'='*60}\n")

    # XML íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    xml_files = list(xml_folder.glob("*.xml"))
    xml_files = [f for f in xml_files if not f.name.startswith(".")]

    # ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ íŒŒì¼ë§Œ í•„í„°ë§
    remaining_files = [f for f in xml_files if f.name not in processed_files_set]

    print(f"ğŸ“Š ì „ì²´ íŒŒì¼: {len(xml_files)}ê°œ")
    print(f"âœ… ì´ë¯¸ ì²˜ë¦¬ë¨: {len(processed_files_set)}ê°œ")
    print(f"â³ ë‚¨ì€ íŒŒì¼: {len(remaining_files)}ê°œ\n")

    if not remaining_files:
        print("âœ… ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        sys.exit(0)

    # ë‚¨ì€ íŒŒì¼ ì²˜ë¦¬
    for idx, xml_file in enumerate(remaining_files, 1):
        print(f"\n[{idx}/{len(remaining_files)}] ì²˜ë¦¬ ì¤‘...")

        result = process_single_xml(xml_file)

        if result["success"]:
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress["processed_files"].append(xml_file.name)
            progress["total_processed"] += 1
            progress["total_chunks"] += result["chunks"]

            # 10ê°œë§ˆë‹¤ ìë™ ì €ì¥
            if progress["total_processed"] % 10 == 0:
                save_progress(progress)

        # ìµœì¢… ì €ì¥
        if idx == len(remaining_files):
            save_progress(progress)

    # ìµœì¢… ê²°ê³¼
    print(f"\n{'='*60}")
    print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"âœ… ì´ ì²˜ë¦¬ëœ íŒŒì¼: {progress['total_processed']}ê°œ")
    print(f"ğŸ“¦ ì´ ìƒì„±ëœ ì²­í¬: {progress['total_chunks']:,}ê°œ")
    print(f"{'='*60}\n")
